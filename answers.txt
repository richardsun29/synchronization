1.1:

1. It takes many threads or iterations because there is a race condition in the
add function: before one thread sets the counter variable to a new value,
another thread might read the counter variable and get its old value, modifying
the counter variable using the old value instead of the new one.  So if two
different threads both increase the counter at the same time, it's possible
that the counter variable only actually increments once.  This only happens
with two or more threads.

2. The race condition could happen during any iteration, so with a
significantly small number of iterations, there are much fewer iterations
during which this race condition could occur, so it's more likely that the
program will succeed.


1.2:

1. Creating the threads is a constant overhead, so when we run the program with
more iterations, the overhead of creating threads takes up a smaller percentage
of the total work, so the program's throughput increases.

2. Make the number of iterations very large so that the overhead of creating
threads is negligible compared to the time spent on the operations.

3. --yield is much slower because a lot of time is being spent doing context
switches from one thread to another.

4. We cannot get valid timings if using --yield because even if there's only
one thread, pthread_yield() causes that single thread to give up control of the
CPU, even though the scheduler wll just rerun the thread, and this adds
overhead to the thread each time it calls the add function.


1.3:

1. All the options perform similarly for low numbers of threads because
not much time is spent by the threads waiting to run the critical section
in the add function. Only one thread can run the critical section at a time,
so with a low number of threads, there are not many threads that have to be
blocked.

2. The three protected operations slow down as the number of threads rises
because only one thread can run the critical section at a time,
so it has to block a large number of threads from running, so all of the
threads spent more time waiting to run.

3. Spin locks are expensive for large numbers of threads because all of the
threads are kept alive and they all use CPU time instead of being blocked,
so much more of the CPU time is wasted on threads waiting to run.


2.1:

1. The time per operation decreases to a point, then increases.


2.2:

1.


2.3:

1.

2.


3-1:

1. The mutex must be held when pthread_cond_wait is called since
pthread_cond_wait will proceed to unlock the mutex. So if one thread holds
the mutex and is running its critical section, and a second thread calls
pthread_cond_wait, then it prematurely unlocks the mutex before the first thread
is finished its critical section, and a third thread that was blocked can now
run its own critical section, causing race conditions.

2. The mutex must be released when the waiting thread is blocked
because if it keeps holding the mutex when it becomes blocked, then all the other
threads wouldn't be able to run their critical sections because the mutex is locked.
If the other threads' critical sections contain code to wake up the blocked thread that called
pthread_cond_wait, then the thread that called pthread_cond_wait won't be able to wake up
since those other threads will be blocked too, so deadlock occurs.

3. The mutex must be reacquired when the calling thread continues because
the calling thread must make sure that the other threads are still not running
their critical sections, and the calling thread should still block if the other threads
haven't released the mutex yet. Also, the calling thread will run more of its critical section,
so it must hold the mutex, not allowing other threads to run their critical sections.

4. The actions of pthread_mutex_unlock and pthread_cond_wait must be done
atomically. If the user releases the mutex before calling pthread_cond_wait,
there is a chance that the process will be interrupted before the
pthread_cond_wait call is executed. Then, another process or thread can acquire
the mutex and perhaps do something that should trigger the user process to wake
up. However, since pthread_cond_wait was never called, the process will still
be asleep, leading to errors.

5. pthread_cond_wait can only be implemented by a system call because it needs
to atomically do two actions: release the mutex, and block. The user can do
this with two function calls, but that creates the race condition that a
context switch occurs between the two function calls. The race condition cannot
be prevented with another spinlock or mutex, which would be acquired before
releasing the other mutex and released after yielding. The caller of
pthread_cond_signal would need to acquire both locks in this case. However, by
the time the caller of pthread_cond_wait can release the second lock, it has
already yielded, defeating the purpose of the function.
