1.1:

1. It takes many threads or iterations because there is a race condition in the
add function: before one thread sets the counter variable to a new value,
another thread might read the counter variable and get its old value, modifying
the counter variable using the old value instead of the new one.  So if two
different threads both increase the counter at the same time, it's possible
that the counter variable only actually increments once.  This only happens
with two or more threads.

2. The race condition could happen during any iteration, so with a
significantly small number of iterations, there are much fewer iterations
during which this race condition could occur, so it's more likely that the
program will succeed.


1.2:

1. Creating the threads is a constant overhead, so when we run the program with
more iterations, the overhead of creating threads takes up a smaller percentage
of the total work, so the program's throughput increases.

2. Make the number of iterations very large so that the overhead of creating
threads is negligible compared to the time spent on the operations.

3. --yield is much slower because a lot of time is being spent doing context
switches from one thread to another.

4. We cannot get valid timings if using --yield because even if there's only
one thread, pthread_yield() causes that single thread to give up control of the
CPU, even though the scheduler wll just rerun the thread, and this adds
overhead to the thread each time it calls the add function.


1.3:

1. All the options perform similarly for low numbers of threads because not
much time is spent by the threads waiting to run the critical section in the
add function. Only one thread can run the critical section at a time, so with a
low number of threads, there are not many threads that have to be blocked.

2. The three protected operations slow down as the number of threads rises
because only one thread can run the critical section at a time, so it has to
block a large number of threads from running, so all of the threads spent more
time waiting to run.

3. Spin locks are expensive for large numbers of threads because all of the
threads are kept alive and they all use CPU time instead of being blocked, so
much more of the CPU time is wasted on threads waiting to run.


2.1:

1. The time per operation increases linearly as a function of the number of
iterations. This is because each iteration increases the length of the linked
list, so, on average, inserts and lookups are O(n/2), and finding the length is
O(n). To correct for this, iterating through the linked list is counted as
operations; the total number of operations is multiplied by half the length of
a linked list, giving the exponential curve seen in the graph.


2.2:

1. In part 2, the time per operation increases faster with respect to number of
threads than in part 1. In part 1, the critical section is only a few
instructions: read the counter, increment, and write the counter. But in part
2, the critical sections are much larger because it includes iterating through
a sorted list. Thus, with larger numbers of threads, each thread has to wait
much longer before it can execute its critical section.


2.3:

1. The time per operation increases as a function of the threads to lists
ratio. Each list has its own mutex or spinlock, so separate threads can access
different lists in isolation. Also, the items are distributed among several
linked lists, so lookups and inserts are now O(n/2 / number of lists).

2. The time per operation depends on both the number of lists and number of
threads. The metric of time:list ratio is interesting because it gives the
average number of threads attempting to access a linked list, which takes into
account the locking mechanisms. If there are many threads but also many lists,
the threads can run mostly in parallel because they are accessing diffrent
sorted lists.


3-1:

1. The mutex must be held when pthread_cond_wait is called since
pthread_cond_wait will proceed to unlock the mutex. So if one thread holds the
mutex and is running its critical section, and a second thread calls
pthread_cond_wait, then it prematurely unlocks the mutex before the first
thread is finished its critical section, and a third thread that was blocked
can now run its own critical section, causing race conditions.

2. The mutex must be released when the waiting thread is blocked because if it
keeps holding the mutex when it becomes blocked, then all the other threads
wouldn't be able to run their critical sections because the mutex is locked.
If the other threads' critical sections contain code to wake up the blocked
thread that called pthread_cond_wait, then the thread that called
pthread_cond_wait won't be able to wake up since those other threads will be
blocked too, so deadlock occurs.

3. The mutex must be reacquired when the calling thread continues because the
calling thread must make sure that the other threads are still not running
their critical sections, and the calling thread should still block if the other
threads haven't released the mutex yet. Also, the calling thread will run more
of its critical section, so it must hold the mutex, not allowing other threads
to run their critical sections.

4. The actions of pthread_mutex_unlock and pthread_cond_wait must be done
atomically. If the user releases the mutex before calling pthread_cond_wait,
there is a chance that the process will be interrupted before the
pthread_cond_wait call is executed. Then, another process or thread can acquire
the mutex and perhaps do something that should trigger the user process to wake
up. However, since pthread_cond_wait was never called, the process will still
be asleep, leading to errors.

5. pthread_cond_wait can only be implemented by a system call because it needs
to atomically do two actions: release the mutex, and block. The user can do
this with two function calls, but that creates the race condition that a
context switch occurs between the two function calls. The race condition cannot
be prevented with another spinlock or mutex, which would be acquired before
releasing the other mutex and released after yielding. The caller of
pthread_cond_signal would need to acquire both locks in this case. However, by
the time the caller of pthread_cond_wait can release the second lock, it has
already yielded, defeating the purpose of the function.
